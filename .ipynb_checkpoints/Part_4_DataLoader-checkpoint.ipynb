{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example before had a relatively small data set, and completing thousands of epochs did not take much computational time at all. However, massive increases in training size and layers make this task difficult enough that we must be attentive to the process. \n",
    "\n",
    "__Terminology:__  \n",
    "- Epoch: One forward and one backward pass  \n",
    "- Batch Size: number of training examples in one epoch\n",
    "- Iterations: number of passes, each pass using (batch size) number of training examples  \n",
    "\n",
    "__Example:__  \n",
    "If you have 1000 training examples with batch size 500, then it will take 2 iterations to complete 1 epoch.\n",
    "  \n",
    "__Basically__, we want to split our data up into batches so that we can control our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Class builds dataset using Data/diabetes.csv\n",
    "class DiabetesDataset():\n",
    "    \"\"\" Diabetes dataset\"\"\"\n",
    "    \n",
    "    # Initialize data\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('Data/diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:,0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:,[-1]])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return(self.x_data[index], self.y_data[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Build dataset and batches using DataLoader\n",
    "dataset = DiabetesDataset()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Build Linear Model, BCE Loss, and Rprop Optimizer\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8,6)\n",
    "        self.l2 = torch.nn.Linear(6,4)\n",
    "        self.l3 = torch.nn.Linear(4,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return(y_pred)\n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.7161743640899658\n",
      "0 1 0.6987360119819641\n",
      "0 2 0.6940663456916809\n",
      "0 3 0.7036055326461792\n",
      "0 4 0.7039104104042053\n",
      "0 5 0.6963328719139099\n",
      "0 6 0.6957908272743225\n",
      "0 7 0.7073799967765808\n",
      "0 8 0.6985552310943604\n",
      "0 9 0.6960105895996094\n",
      "0 10 0.7038606405258179\n",
      "0 11 0.7052321434020996\n",
      "0 12 0.7052555680274963\n",
      "0 13 0.7039276957511902\n",
      "0 14 0.7044946551322937\n",
      "0 15 0.7100305557250977\n",
      "0 16 0.6994779109954834\n",
      "0 17 0.699142336845398\n",
      "0 18 0.6976528763771057\n",
      "0 19 0.7003467679023743\n",
      "0 20 0.69708651304245\n",
      "0 21 0.6975420713424683\n",
      "0 22 0.6976831555366516\n",
      "0 23 0.695855438709259\n",
      "0 24 0.6971123218536377\n",
      "0 25 0.6934838891029358\n",
      "0 26 0.6942680478096008\n",
      "0 27 0.6933479309082031\n",
      "0 28 0.6923757195472717\n",
      "0 29 0.6922417283058167\n",
      "0 30 0.6921616792678833\n",
      "0 31 0.6908133029937744\n",
      "0 32 0.6891487836837769\n",
      "0 33 0.6944347620010376\n",
      "0 34 0.6898872256278992\n",
      "0 35 0.6884573101997375\n",
      "0 36 0.6898629069328308\n",
      "0 37 0.6873884201049805\n",
      "0 38 0.6875864863395691\n",
      "0 39 0.6845865845680237\n",
      "0 40 0.6866496801376343\n",
      "0 41 0.689348578453064\n",
      "0 42 0.6878235340118408\n",
      "0 43 0.6943655610084534\n",
      "0 44 0.6905441284179688\n",
      "0 45 0.6783336997032166\n",
      "0 46 0.69185870885849\n",
      "0 47 0.6783112287521362\n",
      "1 0 0.6809571981430054\n",
      "1 1 0.6835750937461853\n",
      "1 2 0.6755896210670471\n",
      "1 3 0.68051677942276\n",
      "1 4 0.6712248921394348\n",
      "1 5 0.6840013861656189\n",
      "1 6 0.6813333630561829\n",
      "1 7 0.6758971810340881\n",
      "1 8 0.6752238869667053\n",
      "1 9 0.6771984100341797\n",
      "1 10 0.6797640919685364\n",
      "1 11 0.6790546178817749\n",
      "1 12 0.6790978312492371\n",
      "1 13 0.6815196871757507\n",
      "1 14 0.6751676201820374\n",
      "1 15 0.6782918572425842\n",
      "1 16 0.6815142035484314\n",
      "1 17 0.6940373778343201\n",
      "1 18 0.6704955101013184\n",
      "1 19 0.6732497215270996\n",
      "1 20 0.6841064691543579\n",
      "1 21 0.6836453676223755\n",
      "1 22 0.6798562407493591\n",
      "1 23 0.6606780290603638\n",
      "1 24 0.6715109348297119\n",
      "1 25 0.6550462245941162\n",
      "1 26 0.6743451952934265\n",
      "1 27 0.6701837778091431\n",
      "1 28 0.6779811382293701\n",
      "1 29 0.6865476369857788\n",
      "1 30 0.6644850373268127\n",
      "1 31 0.6820431351661682\n",
      "1 32 0.6822479367256165\n",
      "1 33 0.6822044849395752\n",
      "1 34 0.6637147068977356\n",
      "1 35 0.6630952954292297\n",
      "1 36 0.7055835723876953\n",
      "1 37 0.7052078247070312\n",
      "1 38 0.6722732186317444\n",
      "1 39 0.6762599945068359\n",
      "1 40 0.6672143936157227\n",
      "1 41 0.6866139769554138\n",
      "1 42 0.6811460852622986\n",
      "1 43 0.691274881362915\n",
      "1 44 0.6666255593299866\n",
      "1 45 0.656014084815979\n",
      "1 46 0.6757636666297913\n",
      "1 47 0.6607869863510132\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels=data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        y_pred = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
